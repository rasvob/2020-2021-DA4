{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - Exercise 3\n",
    "This lecture is about basics of the time series forecasting. We will discuss the natural gas consumption forecasting topic using provided dataset.\n",
    "\n",
    "Raw dataset is available at [vsb.ai](https://vsb.ai/natural-gas-forecasting), but we will use already pre-processed version of it. You can download the simplified version from our [Github](https://github.com/rasvob/2020-2021-DA4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/2020-2021-DA4/blob/master/03_Forecasting.ipynb)\n",
    "\n",
    "[Download from Github](https://github.com/rasvob/2020-2021-DA4/blob/master/03_Forecasting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start with importing commonly used libraries. \n",
    "Nothing new here, I assume that you already know most of them.\n",
    "## We will use maily Plotly (you can see [this link](https://plotly.com/python/plotly-express/) for more information) for our visualizations.\n",
    "Plotly is a nice alternative to Matplotlib or Seaborn as it uses Javascript-backed plots with very straightforward API and basic interactivity out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "# from plotly.offline import init_notebook_mode\n",
    "# init_notebook_mode(connected=False)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y statsmodels\n",
    "!pip install statsmodels==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf, ccf, ccovf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install the rstl dependency from pip for the time series decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rstl\n",
    "import rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.arima.model import ARIMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared common metrics for model evaluation beforehand. We will use these functions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes MAPE\n",
    "\"\"\"\n",
    "def mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes SMAPE\n",
    "\"\"\"\n",
    "def symetric_mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.mean(np.abs((y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred))/2.0))) * 100\n",
    "\n",
    "\"\"\"\n",
    "Computes MAE, MSE, MAPE, SMAPE, R2\n",
    "\"\"\"\n",
    "def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    y_true, y_pred = df['y_true'].values, df['y_pred'].values\n",
    "    return compute_metrics_raw(y_true, y_pred)\n",
    "\n",
    "def compute_metrics_raw(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n",
    "    mae, mse, mape, smape, r2 = mean_absolute_error(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), symetric_mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    return pd.DataFrame.from_records([{'MAE': mae, 'MSE': mse, 'MAPE': mape, 'SMAPE': smape, 'R2': r2}], index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/rasvob/2020-2021-DA4/master/datasets/ppnet_metar_v8_MAD.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will drop year 2019 for now and use only years 2013 to 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Year < 2019].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'TestSet'] = 0\n",
    "df.loc[df.Year == 2018, 'TestSet'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset covers six whole years from January 1, 2013 to December 31, 2018. All data features are available at an hourly frequency. The whole dataset is composed of 52,584 data points. These data points were assembled from three main components.\n",
    "\n",
    "The first component was created from consumption data. Prague is the capital city of the Czech Republic and its distribution network consisted of 422,926 customers in 2018. Total consumption was 3.82 billion m3. Residential sector included 381,914 households (33.3% of consumption). Industrial sector consisted from 177 big (24.8% of consumption), 39,175 medium (18.9% of consumption) and 1,652 small customers (21.9% of consumption). Missing remainder to 100% were operational losses that occurred during distribution, e.g., pipeline leak. The heating season in the Czech Republic is from September 1 to May 31. Usually, it is required for the heating season to begin that the temperature drops below +13 Â°C for two consecutive days, and no warming is forecasted for the following days. The heating season usually represents about 70% - 75% of the whole year's consumption.\n",
    "\n",
    "The second component includes weather variables. We have used data from the Prague LKPR airport weather station. Airports are required to periodically issue METAR (aerodrome routine meteorological report) information. Those reports are archived and preserved for a long time.\n",
    "\n",
    "The third component representing economic features are natural gas price data. We have obtained price data from the Czech energy regulation office and included them in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have multiple features in the dataset. Consumption is the forecasted endogenous variable, other features are treated as exogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have circa 52k datapoints which should be sufficient even for very complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the viz' below. Do you see any patterns in the data?\n",
    "- Note: You can zoom to see more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df['Consumption'], x=df.index, color=df.Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df['Consumption'], x=df.index, color=df.TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about now? Can we make any assumptions about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, y='Consumption', color='Month', facet_row='TestSet')\n",
    "fig.update_layout(\n",
    "    height=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have no missing values. Can you think about some easy ways to deal with the missing data if we had the issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to split the data into two parts. X is the model input and y is the output - gas consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:, df.columns != 'Consumption'].copy(), df.Consumption.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is very important to check if the time series is self-correlated. We will use auto-correlation function for this task which computes correlation between raw values and lagged ones.\n",
    "\n",
    "### What patterns do you see in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_acf = acf(y, nlags=172)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[i for i in range(0, len(res_acf))], y=res_acf, mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can take a look at the dependency of the consumption variable on the other exogenous factors as well. We will use cross-correlation function for this purpose.\n",
    "#### What seems important to you based on the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ccf = ccf(x=X.Temperature, y=y, unbiased=True)\n",
    "res_ccf.shape\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[i for i in range(0, len(res_ccf[:168*2]))], y=res_ccf[:168*2], mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ccf = ccf(x=X.Humidity, y=y, unbiased=True)\n",
    "res_ccf.shape\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[i for i in range(0, len(res_ccf[:168*2]))], y=res_ccf[:168*2], mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ccf = ccf(x=X.Wind_speed, y=y, unbiased=True)\n",
    "res_ccf.shape\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[i for i in range(0, len(res_ccf[:168*2]))], y=res_ccf[:168*2], mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will use 24h long forecast horizon. This length will be utilized in other examples as well.\n",
    "\n",
    "## We will start with something simple.\n",
    "You probably remember simple autoregressive model (AR) from the monday lecture. We will start with AR(24) model, which means that the forecasted value will be a linear combination of the 24 previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[X.TestSet == 0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y[X.TestSet == 1].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will fit the model on training data at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(y_train.asfreq('H'),order=(24, 0, 0), trend='n', freq='H', enforce_stationarity=True, enforce_invertibility=True)\n",
    "res = model.fit()\n",
    "res = res.append(endog=y_test[0:1].asfreq('H'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = res.forecast(24)\n",
    "forecasts = y_pred.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we need to feed the true consumption values for the last 24 hours into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 365):\n",
    "    if 1 + (i+1)*24 > len(y_test):\n",
    "        break\n",
    "    res = res.extend(endog=y_test[1 + i*24:1 + (i+1)*24].asfreq('H'))\n",
    "    y_pred = res.forecast(24)\n",
    "    forecasts = forecasts.append(y_pred)\n",
    "\n",
    "# This is just for alignment of the true and forecasted time series\n",
    "y_pred = forecasts[:-1]\n",
    "y_test = y_test[1:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can compute the model error metrics and vizualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred}, index=y_test.index)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We were able to obtain relatively good results even with model that simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_s = df_res.stack().reset_index().rename({'level_1': 'Type', 0: 'Value'}, axis=1)\n",
    "px.line(df_res_s, y='Value', x='Datetime', color='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AR model utilizes only a single variable, it is an univariate model after all, but we have more covariate time series available.\n",
    "### We will now move to more complex models based on machine learning algorithms.\n",
    "### We will utilize only a single model for the all 24 forecasts and we will do the forecasting in the direct manner, i.e. no interdependency among multiple forecasted values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset needs to be built for the ML model with lagged variable values as features.\n",
    "### You are free to use any variables you want. We will demonstrate the usage only with a subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.loc[:, df.columns != 'Consumption'].copy(), df.Consumption.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trange(24, 169):\n",
    "    X.loc[:, f'Consumption_lag_{x}'] = y.shift(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trange(24, 24*2+1):\n",
    "    X.loc[:, f'Temperature_lag_{x}'] = X['Temperature'].shift(x)\n",
    "    X.loc[:, f'Humidity_lag_{x}'] = X['Humidity'].shift(x)\n",
    "    X.loc[:, f'Cena_lag_{x}'] = X['Cena_bfill'].shift(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trange(167, 169):\n",
    "    X.loc[:, f'Temperature_lag_{x}'] = X.loc[:, 'Temperature'].shift(x)\n",
    "    X.loc[:, f'Humidity_lag_{x}'] = X.loc[:, 'Humidity'].shift(x)\n",
    "    X.loc[:, f'Cena_lag_{x}'] = X.loc[:, 'Cena_bfill'].shift(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following technique is called cyclical-encoding. \n",
    "Imagine that you have days from Monday to Sunday encoded with numbers from 1 to 7. Tuesday is one day after Monday, thus 2 is after 1 and 2 - 1 = 1 day difference. This holds through whole week except for Sunday, because 1 does not go after the 7 according to the used encoding scheme. We can deal with the situation with mapping days of the week (and other time related information) into 2D space using goniometric functions. These functions are periodic and distance of Sunday and Monday is the same as in the other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, 'Day_of_week_sin'] = np.sin(2 * np.pi * X['Day_of_week']/X['Day_of_week'].max())\n",
    "X.loc[:, 'Day_of_week_cos'] = np.cos(2 * np.pi * X['Day_of_week']/X['Day_of_week'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, 'Month_sin'] = np.sin(2 * np.pi * X['Month']/X['Month'].max())\n",
    "X.loc[:, 'Month_cos'] = np.cos(2 * np.pi * X['Month']/X['Month'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, 'Hour_sin'] = np.sin(2 * np.pi * X['Hour']/X['Hour'].max())\n",
    "X.loc[:, 'Hour_cos'] = np.cos(2 * np.pi * X['Hour']/X['Hour'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, 'Day_sin'] = np.sin(2 * np.pi * X['Day']/X['Day'].max())\n",
    "X.loc[:, 'Day_cos'] = np.cos(2 * np.pi * X['Day']/X['Day'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can feature engineer even more exogenous variables. We can for example mark certain months as a summer season explicitly.\n",
    "- We saw in the beggining of the lecture that the consumption is very low during the summer compared to the other seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['IsSummer'] = 0\n",
    "X.loc[X.Month.between(6, 8), 'IsSummer'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can imagine that people turn on the heating in their homes based on the month and temperature. We can mark the probable heating season with the so-called dummy variable.\n",
    "- This is simplified variant of a heating season definition, there are multiple ways how to do this.\n",
    "- We can mark weekend period as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['IsHeatingSeason'] = 1\n",
    "X.loc[X.Month.between(6, 8), 'IsHeatingSeason'] = 0\n",
    "heat_final = X.apply(lambda x: 1 if x['Temperature'] < 13 and x['IsHeatingSeason'] == 1 else 0, axis=1)\n",
    "X['IsHeatingSeason'] = heat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['IsWeekend'] = 0\n",
    "X.loc[X.Day_of_week.between(6, 7), 'IsWeekend'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can't use one model for multiple outputs in case of a tree-based algorithm. \n",
    "#### We will use the methodology we defined in our paper [Short-term natural gas consumption forecasting from long-term data collection](https://doi.org/10.1016/j.energy.2020.119430) which uses the difference from a fixed point in time as an output of the forecasting model.\n",
    "#### Midnight is this fixed point in our case. So every forecasted value will be treated difference as a difference from the last midnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First mark the midnights rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ResetSignal'] = 0\n",
    "X.loc[X.Hour == 0, 'ResetSignal'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the new column in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Residual_diff_from_midnight'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Residual'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight = X.Residual[0]\n",
    "col_idx = X.columns.get_loc('Residual_diff_from_midnight')\n",
    "for row_idx in trange(1, X.shape[0]):\n",
    "    row = X.iloc[row_idx]\n",
    "    val = row.Residual - midnight\n",
    "    X.iloc[row_idx, col_idx] = val\n",
    "    if row.ResetSignal == 1:\n",
    "        midnight = row.Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=X.index, y=X['Residual_diff_from_midnight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(24, 24*2+1):\n",
    "    X.loc[:, f'Residual_diff_from_midnight_lag_{x}'] = X['Residual_diff_from_midnight'].shift(x)\n",
    "    \n",
    "for x in range(167, 169):\n",
    "    X.loc[:, f'Residual_diff_from_midnight_lag_{x}'] = X['Residual_diff_from_midnight'].shift(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.Residual_diff_from_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[X.TestSet == 0], X[X.TestSet == 1], y[X.TestSet == 0], y[X.TestSet == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to drop certain features so we don't have information leak in our model. \n",
    "- E.g. current temperature\n",
    "- Some rows need to be dropped as well - we don't know previous consumption values for the first 24 rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_features_nona = X_train.drop(['Residual_diff_from_midnight', 'Year', 'Month', 'Day', 'Hour', 'Day_of_week','Temperature', 'Pressure', 'Pressure2', 'Humidity', 'Wind_direction', 'Wind_speed', 'Phenomena', 'Recent_phenomena', 'Visibility', 'Dewpoint', 'Datetime.1', 'Clouds_low_text', 'Clouds_low_m', 'Clouds_medium_text', 'Clouds_medium_m', 'Clouds_high_text', 'Clouds_high_m', 'Residual', 'ResetSignal', 'IsMissing', 'Cena_bfill', 'TestSet'], axis=1).dropna()\n",
    "X_test_selected_features_nona = X_test.drop(['Residual_diff_from_midnight', 'Year', 'Month', 'Day', 'Hour', 'Day_of_week','Temperature', 'Pressure', 'Pressure2', 'Humidity', 'Wind_direction', 'Wind_speed', 'Phenomena', 'Recent_phenomena', 'Visibility', 'Dewpoint', 'Datetime.1', 'Clouds_low_text', 'Clouds_low_m', 'Clouds_medium_text', 'Clouds_medium_m', 'Clouds_high_text', 'Clouds_high_m', 'Residual', 'ResetSignal', 'IsMissing', 'Cena_bfill', 'TestSet'], axis=1).dropna()\n",
    "y_train_no_na = y_train.dropna()\n",
    "y_test_no_na = y_test.dropna()\n",
    "y_train_no_na = y_train_no_na[y_train_no_na.index.isin(X_train_selected_features_nona.index)]\n",
    "X_test_selected_features_nona = X_test_selected_features_nona[X_test_selected_features_nona.index.isin(y_test_no_na.index)]\n",
    "X_train_selected_features_nona.shape, y_train_no_na.shape, X_test_selected_features_nona.shape, y_test_no_na.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset is ready so we can train the model now. We will use Light-gradient boosting algorithm as our model. You can choose any ML algorithm you want, e.g. Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = LGBMRegressor(n_estimators=200, n_jobs=4, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.fit(X_train_selected_features_nona, y_train_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = alg.predict(X_test_selected_features_nona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can take a look at the forecasted values by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'y_true': y_test_no_na.values, 'y_pred': y_pred}, index=y_test_no_na.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_s = df_res.stack().reset_index().rename({'level_1': 'Type', 0: 'Value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_res_s, y='Value', x='Datetime', color='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that lagged values which were day/week before are the most valuable as well as sin/cos encoded time features and our engineered dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_imp = pd.DataFrame({'FeatureName': X_train_selected_features_nona.columns, 'FeatureImportance': alg.feature_importances_}).sort_values(by='FeatureImportance', ascending=False)\n",
    "px.bar(df_feat_imp.sort_values(by='FeatureImportance', ascending=False).iloc[:15, :], y='FeatureName', x='FeatureImportance', orientation='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, we have forecasted values but what now?\n",
    "Remember that we are forecasting the difference from last midnight. The next step is reconstruction of the data, so we obtain raw natural gas consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_midnight = X[X.TestSet == 1].apply(lambda x: x['Residual'] if x['Hour'] == 0 else np.nan, axis=1).ffill().shift(1)\n",
    "y_test_midnight['2018-01-01 00:00:00'] = X[X.index == '2017-12-31 00:00:00'].Residual\n",
    "y_test_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_pred = pd.Series(y_pred, index=y_test_no_na.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_pred = ps_y_pred + y_test_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = df[df.TestSet == 1].Consumption\n",
    "orig_data.index = pd.DatetimeIndex(orig_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res = pd.DataFrame({'y_true': orig_data, 'y_pred': ps_y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can compute the error measurement metrics and vizualize our results in this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_res.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_s = df_res.stack().reset_index().rename({'level_1': 'Type', 0: 'Value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_res_s, y='Value', x='Datetime', color='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we assume about the error distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_err = df_res.apply(lambda x: x[0] - x[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=ps_err.index, y=ps_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are interested in the daily results, it is no problem to aggregate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_daily = df_res.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_s_daily = df_res_daily.stack().reset_index().rename({'level_1': 'Type', 0: 'Value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_res_s_daily, y='Value', x='Datetime', color='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_res_daily.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can try another preprocessing techniques. The first one is called differencing.\n",
    "### Do you remember how it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Residual_diff_from_midnight'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why diff by 24 hours? Think about the ACF plot and forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = df.Consumption.diff(24).dropna()\n",
    "X['Residual'] = y_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will use the STL decomposition algorithms to create an additive decomposed model from the differenced time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_decomp = rstl.STL(y_diff, freq=24*7, robust=False, s_window='periodic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend, seasonal, residual, weights = stl_decomp.trend, stl_decomp.seasonal, stl_decomp.remainder, stl_decomp.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=y_diff.index, y=trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=y_diff.index, y=seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=y_diff.index, y=residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we take a closer look at the trend part, we can see that there is no trend present in the data and this part is very similiar to the residual one. Moreover it is lower by a magnitude thus we can merge the parts into one.\n",
    "- Just beware that if the data had e.g. close-to-linear trend present, merging it with the residual part is not a good idea. It is better to use some simple separate model, e.g. exponential smoothing, AR, etc., to forecast trend part individualy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_trend = residual + trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, f'Seasonal'] = pd.Series((np.concatenate([[np.nan]*24, seasonal])), index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, f'Residual'] = pd.Series((np.concatenate([[np.nan]*24, residual_trend])), index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[:, f'Trend'] = pd.Series((np.concatenate([[np.nan]*24,trend])), index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_dict = {k:X[(X.TestSet == 0) & (X.Month == k)].groupby(['Day_of_week', 'Hour']).Seasonal.mean().reset_index() for k in  X.Month.value_counts().index.values}\n",
    "\n",
    "def fill_seasonal(month, day, hour, seasonal_dict):\n",
    "    season = seasonal_dict[month]\n",
    "    return season[(season.Day_of_week == day) & (season.Hour == hour)].Seasonal.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[X.TestSet == 1, 'Seasonal'] = X[X.TestSet == 1].loc[:, ['Month', 'Day_of_week', 'Hour']].progress_apply(lambda x: fill_seasonal(x[0], x[1], x[2], seasonal_dict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trange(1, 169):\n",
    "    X.loc[:, f'Seasonal_lag_{x}'] = X['Seasonal'].shift(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trange(24, 169):\n",
    "    X.loc[:, f'Residual_lag_{x}'] = X['Residual'].shift(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight = X.Residual.iloc[0]\n",
    "col_idx = X.columns.get_loc('Residual_diff_from_midnight')\n",
    "for row_idx in trange(1, X.shape[0]):\n",
    "    row = X.iloc[row_idx]\n",
    "    val = row.Residual - midnight\n",
    "    X.iloc[row_idx, col_idx] = val\n",
    "    if row.ResetSignal == 1:\n",
    "        midnight = row.Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does seem the distribution different from the last time? How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=X.index, y=X['Residual_diff_from_midnight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trange(24, 24*2+1):\n",
    "    X.loc[:, f'Residual_diff_from_midnight_lag_{x}'] = X['Residual_diff_from_midnight'].shift(x)\n",
    "    X.loc[:, f'Residual_lag_{x}'] = X['Residual'].shift(x)\n",
    "    \n",
    "for x in range(167, 169):\n",
    "    X.loc[:, f'Residual_diff_from_midnight_lag_{x}'] = X['Residual_diff_from_midnight'].shift(x)\n",
    "    X.loc[:, f'Residual_lag_{x}'] = X['Residual'].shift(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.Residual_diff_from_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[X.TestSet == 0], X[X.TestSet == 1], y[X.TestSet == 0], y[X.TestSet == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to drop certain features so we don't have information leak in our model. \n",
    "- E.g. current temperature\n",
    "- Some rows need to be dropped as well - we don't know previous consumption values for the first 24 rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_features_nona = X_train.drop(['Residual_diff_from_midnight', 'Year', 'Month', 'Day', 'Hour', 'Day_of_week','Temperature', 'Pressure', 'Pressure2', 'Humidity', 'Wind_direction', 'Wind_speed', 'Phenomena', 'Recent_phenomena', 'Visibility', 'Dewpoint', 'Datetime.1', 'Clouds_low_text', 'Clouds_low_m', 'Clouds_medium_text', 'Clouds_medium_m', 'Clouds_high_text', 'Clouds_high_m', 'Residual', 'ResetSignal', 'IsMissing', 'Cena_bfill', 'TestSet'], axis=1).dropna()\n",
    "X_test_selected_features_nona = X_test.drop(['Residual_diff_from_midnight', 'Year', 'Month', 'Day', 'Hour', 'Day_of_week','Temperature', 'Pressure', 'Pressure2', 'Humidity', 'Wind_direction', 'Wind_speed', 'Phenomena', 'Recent_phenomena', 'Visibility', 'Dewpoint', 'Datetime.1', 'Clouds_low_text', 'Clouds_low_m', 'Clouds_medium_text', 'Clouds_medium_m', 'Clouds_high_text', 'Clouds_high_m', 'Residual', 'ResetSignal', 'IsMissing', 'Cena_bfill', 'TestSet'], axis=1).dropna()\n",
    "y_train_no_na = y_train.dropna()\n",
    "y_test_no_na = y_test.dropna()\n",
    "y_train_no_na = y_train_no_na[y_train_no_na.index.isin(X_train_selected_features_nona.index)]\n",
    "X_test_selected_features_nona = X_test_selected_features_nona[X_test_selected_features_nona.index.isin(y_test_no_na.index)]\n",
    "X_train_selected_features_nona.shape, y_train_no_na.shape, X_test_selected_features_nona.shape, y_test_no_na.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = LGBMRegressor(n_estimators=100, n_jobs=8, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.fit(X_train_selected_features_nona, y_train_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = alg.predict(X_test_selected_features_nona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can take a look at the forecasted values by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'y_true': y_test_no_na.values, 'y_pred': y_pred}, index=y_test_no_na.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_midnight = X[X.TestSet == 1].apply(lambda x: x['Residual'] if x['Hour'] == 0 else np.nan, axis=1).ffill().shift(1)\n",
    "y_test_midnight['2018-01-01 00:00:00'] = X[X.index == '2017-12-31 00:00:00'].Residual\n",
    "y_test_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_pred = pd.Series(y_pred, index=y_test_no_na.index)\n",
    "ps_y_pred = ps_y_pred + y_test_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_shifted_residuals = (df.Consumption.shift(24)[(df.index >= '2018-01-01 00:00:00') & (df.index <= '2018-12-31 23:00:00')] + ps_y_pred) + X_test.Seasonal\n",
    "ps_shifted_residuals.index = pd.DatetimeIndex(ps_y_pred.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = df[df.TestSet == 1].Consumption\n",
    "orig_data.index = pd.DatetimeIndex(orig_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({'y_true': orig_data, 'y_pred': ps_shifted_residuals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can compute the error measurement metrics and vizualize our results in this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_res.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_s = df_res.stack().reset_index().rename({'level_1': 'Type', 0: 'Value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_res_s, y='Value', x='Datetime', color='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we can imagine that the model could be re-trained during the year 2018 so it reflects some of the recent changes in the data.\n",
    "- We can try to train 12 different models, one for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_features_nona.index = pd.DatetimeIndex(X_train_selected_features_nona.index)\n",
    "X_test_selected_features_nona.index = pd.DatetimeIndex(X_test_selected_features_nona.index)\n",
    "y_train_no_na.index = pd.DatetimeIndex(y_train_no_na.index)\n",
    "y_test_no_na.index = pd.DatetimeIndex(y_test_no_na.index)\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df_res_final = pd.DataFrame({'y_true': df[df.TestSet == 1].Consumption, 'y_pred': np.nan}, index=pd.DatetimeIndex(y_test_no_na.index))\n",
    "y_test_midnight = X[X.TestSet == 1].apply(lambda x: x['Residual'] if x['Hour'] == 0 else np.nan, axis=1).ffill().shift(1)\n",
    "y_test_midnight['2018-01-01 00:00:00'] = X[X.index == '2017-12-31 00:00:00'].Residual\n",
    "y_test_midnight.index = pd.DatetimeIndex(y_test_midnight.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = LGBMRegressor(n_estimators=100, n_jobs=8, random_state=14)\n",
    "alg.fit(X_train_selected_features_nona, y_train_no_na)\n",
    "y_pred = alg.predict(X_test_selected_features_nona[X_test_selected_features_nona.index.month == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_pred = pd.Series(y_pred, index=y_test_no_na[y_test_no_na.index.month == 1].index)\n",
    "ps_y_pred = ps_y_pred + y_test_midnight[y_test_midnight.index.month == 1]\n",
    "ps_shifted_residuals = (df.Consumption.shift(24)[ps_y_pred.index] + ps_y_pred) + X_test_selected_features_nona.loc[ps_y_pred.index, 'Seasonal']\n",
    "df_res_final.loc[ps_y_pred.index, 'y_pred'] = ps_shifted_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extended, y_train_extended = X_train_selected_features_nona.copy(), y_train_no_na.copy()\n",
    "for x in trange(1, 12):\n",
    "    X_train_extended = pd.concat([X_train_extended, X_test_selected_features_nona[X_test_selected_features_nona.index.month == x]])\n",
    "    y_train_extended = pd.concat([y_train_extended, y_test_no_na[y_test_no_na.index.month == x]])\n",
    "    next_month = x+1\n",
    "    alg = LGBMRegressor(n_estimators=100, n_jobs=8, random_state=14)\n",
    "    alg.fit(X_train_extended, y_train_extended)\n",
    "    \n",
    "    y_pred = alg.predict(X_test_selected_features_nona[X_test_selected_features_nona.index.month == next_month])\n",
    "    ps_y_pred = pd.Series(y_pred, index=y_test_no_na[y_test_no_na.index.month == next_month].index)\n",
    "    ps_y_pred = ps_y_pred + y_test_midnight[y_test_midnight.index.month == next_month]\n",
    "    ps_shifted_residuals = (df.Consumption.shift(24)[ps_y_pred.index] + ps_y_pred) + X_test_selected_features_nona.loc[ps_y_pred.index, 'Seasonal']\n",
    "    df_res_final.loc[ps_y_pred.index, 'y_pred'] = ps_shifted_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_res_final.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_s = df_res_final.stack().reset_index().rename({'level_1': 'Type', 0: 'Value'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_res_s, y='Value', x='Datetime', color='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks for the rest of the lecture\n",
    "1. Try to add more or remove lags of a selected exogenous variables and re-train the model with them.\n",
    "2. Try different algorithms (Random forest or linear regression for example).\n",
    "3. Try to re-train the model by longer or shorter period of time.\n",
    "3. Compare the new model with the original one. Did the MAE, MSE etc changed? If it did, how?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('n_vnev': venv)",
   "language": "python",
   "name": "python37564bitnvnevvenv8dbfbcd44e0c4d4eb1f6bb51139e960f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
